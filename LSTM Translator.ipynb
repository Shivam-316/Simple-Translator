{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow import keras\nimport re\nimport time\nimport seaborn as sns\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":88,"outputs":[{"output_type":"stream","text":"/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data=pd.read_csv('/kaggle/input/hindienglish-corpora/Hindi_English_Truncated_Corpus.csv')","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"(127607, 3)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop('source',axis=1)\ndata","execution_count":91,"outputs":[{"output_type":"execute_result","execution_count":91,"data":{"text/plain":"                                         english_sentence  \\\n0       politicians do not have permission to do what ...   \n1              I'd like to tell you about one such child,   \n2       This percentage is even greater than the perce...   \n3       what we really mean is that they're bad at not...   \n4       .The ending portion of these Vedas is called U...   \n...                                                   ...   \n127602  Examples of art deco construction can be found...   \n127603                          and put it in our cheeks.   \n127604  As for the other derivatives of sulphur , the ...   \n127605  its complicated functioning is defined thus in...   \n127606  They've just won four government contracts to ...   \n\n                                           hindi_sentence  \n0       राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n1       मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n2        यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n3          हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n4             इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n...                                                   ...  \n127602  आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...  \n127603                    और अपने गालों में डाल लेते हैं।  \n127604  जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...  \n127605  Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .  \n127606  हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...  \n\n[127607 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>english_sentence</th>\n      <th>hindi_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>politicians do not have permission to do what ...</td>\n      <td>राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I'd like to tell you about one such child,</td>\n      <td>मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>This percentage is even greater than the perce...</td>\n      <td>यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>what we really mean is that they're bad at not...</td>\n      <td>हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>.The ending portion of these Vedas is called U...</td>\n      <td>इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>127602</th>\n      <td>Examples of art deco construction can be found...</td>\n      <td>आर्ट डेको शैली के निर्माण मैरीन ड्राइव और ओवल ...</td>\n    </tr>\n    <tr>\n      <th>127603</th>\n      <td>and put it in our cheeks.</td>\n      <td>और अपने गालों में डाल लेते हैं।</td>\n    </tr>\n    <tr>\n      <th>127604</th>\n      <td>As for the other derivatives of sulphur , the ...</td>\n      <td>जहां तक गंधक के अन्य उत्पादों का प्रश्न है , द...</td>\n    </tr>\n    <tr>\n      <th>127605</th>\n      <td>its complicated functioning is defined thus in...</td>\n      <td>Zरचना-प्रकिया को उसने एक पहेली में यों बांधा है .</td>\n    </tr>\n    <tr>\n      <th>127606</th>\n      <td>They've just won four government contracts to ...</td>\n      <td>हाल ही में उन्हें सरकारी ठेका मिला है करीब सौ ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>127607 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted_idx=[]\nx=re.compile(r'\\d+')\nfor idx,cols in data.iterrows():\n    try:\n        if x.match(cols['english_sentence']):\n            unwanted_idx.append(idx)\n    except:\n        print(idx)","execution_count":92,"outputs":[{"output_type":"stream","text":"37554\n59804\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(unwanted_idx + [37554,59804],axis=0)","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_d={}\nfor text in data.english_sentence:\n    l=len(text.split(' '))\n    eng_d.setdefault(l,0)\n    eng_d[l]+=1","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_dic={k:v for k,v in sorted(eng_d.items(),key=lambda x: x[1],reverse=True)}","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_dic[10]","execution_count":96,"outputs":[{"output_type":"execute_result","execution_count":96,"data":{"text/plain":"6044"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_d={}\nfor text in data.hindi_sentence:\n    l=len(text.split(' '))\n    hindi_d.setdefault(l,0)\n    hindi_d[l]+=1","execution_count":97,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_dic={k:v for k,v in sorted(hindi_d.items(),key=lambda x: x[1],reverse=True)}","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_dic[10]","execution_count":99,"outputs":[{"output_type":"execute_result","execution_count":99,"data":{"text/plain":"6068"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"unwanted_len_id=[]\nfor idx,cols in data.iterrows():\n    if len(cols.english_sentence.split(' ')) > 10 or len(cols.hindi_sentence.split(' ')) > 10:\n        unwanted_len_id.append(idx)","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.drop(unwanted_len_id,axis=0)","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape","execution_count":102,"outputs":[{"output_type":"execute_result","execution_count":102,"data":{"text/plain":"(45596, 2)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=data.iloc[:25000]\nval=data.iloc[25000:]","execution_count":103,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape,val.shape","execution_count":104,"outputs":[{"output_type":"execute_result","execution_count":104,"data":{"text/plain":"((25000, 2), (20596, 2))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=train.to_numpy()\nval=val.to_numpy()","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"english=train[:,0]\nhindi=train[:,1]","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_and_tokenize(language):\n    pattern=r'[!\"#$%&()*,+-./:;\\[\\]<=>?@\\\\^_`{|}~\\t\\n\\d“”]'\n    language=tf.strings.lower(language)\n    language=tf.strings.regex_replace(language,pattern,'')\n    language=tf.strings.strip(language)\n    \n    lang=[]\n    for text in language:\n        lang.append('<sos> '+ text.numpy().decode('utf-8') + ' <eos>')\n            \n    lang=np.array(lang)\n    tokenizer=keras.preprocessing.text.Tokenizer(filters='',split=' ')\n    tokenizer.fit_on_texts(lang)\n    tensor=tokenizer.texts_to_sequences(lang)\n    \n    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post',value=0)\n\n    return tensor,tokenizer","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_tokenized,eng_tokenizer=preprocess_and_tokenize(english)","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hindi_tokenized,hindi_tokenizer=preprocess_and_tokenize(hindi)","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset=tf.data.Dataset.from_tensor_slices((tf.Variable(eng_tokenized),tf.Variable(hindi_tokenized))).shuffle(10000).batch(128).prefetch(1)","execution_count":110,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Encoder(keras.Model):\n    def __init__(self,vocab_size=10000,emb_dim=128,units=256,batch_size=64):\n        super(Encoder,self).__init__()\n        self.units = units\n        self.batch = batch_size\n        self.emb_layer = keras.layers.Embedding(vocab_size,emb_dim)\n        self.lstm = keras.layers.LSTM(self.units,return_sequences=True,return_state=True)\n        \n    def call(self,x,states):\n        emb=self.emb_layer(x)\n        output,hidden,carry=self.lstm(emb,initial_state=states)\n        return output,hidden,carry\n    \n    def init_hidden_state(self):\n        return tf.zeros((self.batch,self.units)),tf.zeros((self.batch,self.units))","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Decoder(keras.Model):\n    def __init__(self,vocab_size=10000,emb_dim=128,units=256,batch_size=64):\n        super(Decoder,self).__init__()\n        self.units = units\n        self.batch = batch_size\n        self.emb_layer = keras.layers.Embedding(vocab_size,emb_dim)\n        self.lstm = keras.layers.LSTM(self.units,return_sequences=True,return_state=True)\n        self.fc=keras.layers.Dense(vocab_size)\n        \n    def call(self,x,states):\n        emb=self.emb_layer(x)\n        output,hidden,carry=self.lstm(emb,initial_state=states)\n        output=self.fc(output)\n        return output,hidden,carry","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer=keras.optimizers.Adam()\nloss=keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\naccuracy=keras.metrics.SparseCategoricalAccuracy()\ndef loss_fn(true,pred):\n    mask = tf.math.logical_not(tf.math.equal(true, 0))\n    loss_=loss(true,pred)\n    mask=tf.cast(mask,dtype=loss_.dtype)\n    loss_*=mask\n    return tf.reduce_mean(loss_)\n\ndef update_accuracy(true,pred):\n    accuracy.update_state(true,pred)\n    \ndef get_accuracy():\n    accuracy_=accuracy.result().numpy()\n    return accuracy_","execution_count":113,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc_vocab_size=len(eng_tokenizer.index_word) + 1\ndec_vocab_size=len(hindi_tokenizer.index_word) + 1","execution_count":114,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder=Encoder(enc_vocab_size,256,512,128)\ndecoder=Decoder(dec_vocab_size,256,512,128)","execution_count":115,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eng_tokenizer.index_word[0]='<pad>'\neng_tokenizer.word_index['<pad>']=0\nhindi_tokenizer.index_word[0]='<pad>'\nhindi_tokenizer.word_index['<pad>']=0","execution_count":129,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef train(input,target,enc_hidden):\n    loss__=0.0\n    with tf.GradientTape() as tape:\n        enc_output,enc_h,enc_c=encoder(input,enc_hidden)\n        enc_states=[enc_h,enc_c]\n        dec_input=tf.expand_dims(target[:,0],1)\n        \n        for t in range(1,target.shape[1]):\n            dec_output,_,_=decoder(dec_input,enc_states)\n            loss__+=loss_fn(target[:,t],dec_output)\n            #update_accuracy(target[:,t],dec_output)\n            dec_input = tf.expand_dims(target[:, t], 1)\n        \n    batch_loss=loss__/int(target.shape[1])\n    variables = encoder.trainable_variables + decoder.trainable_variables\n    gradients=tape.gradient(loss__,variables)\n        \n    optimizer.apply_gradients(zip(gradients,variables))\n\n    return batch_loss","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs=50\nsteps_per_epoch=25000//128 #batches\nfor epoch in range(epochs):\n    start=time.time()\n    enc_hidden=encoder.init_hidden_state()\n    total_loss=0.0\n    for (batch,(inp,tar)) in enumerate(dataset.take(steps_per_epoch)):\n        batch_loss=train(inp,tar,enc_hidden)\n        total_loss+=batch_loss\n    #print('Epoch {}      Accuray {:.2f}   Avg. Loss {:.4f}'.format(epoch + 1,float(accuracy.result()),total_loss/steps_per_epoch),end='  ')\n    print('Epoch {}      Avg. Loss {:.4f}'.format(epoch + 1,total_loss/steps_per_epoch),end='  ')\n    print('Time Taken: {:.1f} sec'.format(time.time() - start))\n    accuracy.reset_states()","execution_count":131,"outputs":[{"output_type":"stream","text":"Epoch 1      Avg. Loss 3.3055  Time Taken: 30.0 sec\nEpoch 2      Avg. Loss 2.9125  Time Taken: 16.4 sec\nEpoch 3      Avg. Loss 2.8070  Time Taken: 16.9 sec\nEpoch 4      Avg. Loss 2.7082  Time Taken: 15.9 sec\nEpoch 5      Avg. Loss 2.6234  Time Taken: 17.9 sec\nEpoch 6      Avg. Loss 2.5406  Time Taken: 16.0 sec\nEpoch 7      Avg. Loss 2.4434  Time Taken: 16.8 sec\nEpoch 8      Avg. Loss 2.3498  Time Taken: 16.5 sec\nEpoch 9      Avg. Loss 2.2626  Time Taken: 17.2 sec\nEpoch 10      Avg. Loss 2.1860  Time Taken: 15.9 sec\nEpoch 11      Avg. Loss 2.1133  Time Taken: 16.6 sec\nEpoch 12      Avg. Loss 2.0366  Time Taken: 16.7 sec\nEpoch 13      Avg. Loss 1.9648  Time Taken: 17.1 sec\nEpoch 14      Avg. Loss 1.8932  Time Taken: 16.4 sec\nEpoch 15      Avg. Loss 1.8248  Time Taken: 16.8 sec\nEpoch 16      Avg. Loss 1.7535  Time Taken: 16.8 sec\nEpoch 17      Avg. Loss 1.6852  Time Taken: 16.7 sec\nEpoch 18      Avg. Loss 1.6219  Time Taken: 16.0 sec\nEpoch 19      Avg. Loss 1.5596  Time Taken: 17.2 sec\nEpoch 20      Avg. Loss 1.4995  Time Taken: 16.4 sec\nEpoch 21      Avg. Loss 1.4435  Time Taken: 17.0 sec\nEpoch 22      Avg. Loss 1.3905  Time Taken: 15.8 sec\nEpoch 23      Avg. Loss 1.3380  Time Taken: 17.6 sec\nEpoch 24      Avg. Loss 1.2889  Time Taken: 16.7 sec\nEpoch 25      Avg. Loss 1.2418  Time Taken: 16.6 sec\nEpoch 26      Avg. Loss 1.1980  Time Taken: 17.2 sec\nEpoch 27      Avg. Loss 1.1552  Time Taken: 16.6 sec\nEpoch 28      Avg. Loss 1.1140  Time Taken: 16.6 sec\nEpoch 29      Avg. Loss 1.0730  Time Taken: 16.1 sec\nEpoch 30      Avg. Loss 1.0349  Time Taken: 17.6 sec\nEpoch 31      Avg. Loss 0.9963  Time Taken: 15.9 sec\nEpoch 32      Avg. Loss 0.9592  Time Taken: 16.7 sec\nEpoch 33      Avg. Loss 0.9221  Time Taken: 15.9 sec\nEpoch 34      Avg. Loss 0.8870  Time Taken: 17.7 sec\nEpoch 35      Avg. Loss 0.8522  Time Taken: 16.6 sec\nEpoch 36      Avg. Loss 0.8176  Time Taken: 17.3 sec\nEpoch 37      Avg. Loss 0.7848  Time Taken: 17.6 sec\nEpoch 38      Avg. Loss 0.7553  Time Taken: 17.2 sec\nEpoch 39      Avg. Loss 0.7247  Time Taken: 16.4 sec\nEpoch 40      Avg. Loss 0.6955  Time Taken: 17.0 sec\nEpoch 41      Avg. Loss 0.6676  Time Taken: 17.8 sec\nEpoch 42      Avg. Loss 0.6406  Time Taken: 17.1 sec\nEpoch 43      Avg. Loss 0.6154  Time Taken: 16.7 sec\nEpoch 44      Avg. Loss 0.5910  Time Taken: 17.8 sec\nEpoch 45      Avg. Loss 0.5660  Time Taken: 16.9 sec\nEpoch 46      Avg. Loss 0.5427  Time Taken: 17.0 sec\nEpoch 47      Avg. Loss 0.5213  Time Taken: 16.9 sec\nEpoch 48      Avg. Loss 0.5007  Time Taken: 18.4 sec\nEpoch 49      Avg. Loss 0.4792  Time Taken: 17.4 sec\nEpoch 50      Avg. Loss 0.4604  Time Taken: 16.7 sec\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(input):\n    hidden=[tf.zeros((1,512)),tf.zeros((1,512))]\n    _,enc_h,enc_c=encoder(input,hidden)\n    enc_states=[enc_h,enc_c]\n    result=[]\n    dec_input = tf.expand_dims(input[:,0], 0)\n    for t in range(input.shape[1]):\n        dec_output,_,_=decoder(dec_input,enc_states)\n        output_id=tf.math.argmax(dec_output[0],-1)\n        output_id=output_id[0].numpy()\n        if output_id == hindi_tokenizer.word_index['<eos>']:\n            return ' '.join(result)\n        dec_input = tf.expand_dims([output_id], 0)\n        result.append(hindi_tokenizer.index_word[output_id])","execution_count":188,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_input(text):\n    pattern=r'[!\"#$%&()*,+-./:;\\[\\]<=>?@\\\\^_`{|}~\\t\\n\\d“”]'\n    text=tf.strings.lower(text)\n    text=tf.strings.regex_replace(text,pattern,'')\n    text=tf.strings.strip(text)\n    return text.numpy().decode(\"utf-8\") ","execution_count":166,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_translation(input_text):\n    length=12\n    eng_processed=['<sos>',]\n    for i in preprocess_input(input_text).split(' '):\n        if i != '':\n            eng_processed.append(i)\n    eng_processed.append('<eos>')\n    eng_tokenized=[eng_tokenizer.word_index[i] for i in eng_processed]\n    eng_tokenized=tf.keras.preprocessing.sequence.pad_sequences([eng_tokenized],maxlen=length,padding='post',value=0)\n    hindi_pred=predict(eng_tokenized)\n    print('Pred. Hindi: ',hindi_pred,end='\\n\\n')","execution_count":189,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in [1,3,10,15,18,19,21,23,24,26,28,33,34,36,38,45 ]:\n    print('No. ',i)\n    print('English:     ',english[i])\n    print('Hindi:       ',hindi[i])\n    get_translation(str(english[i]))","execution_count":192,"outputs":[{"output_type":"stream","text":"No.  1\nEnglish:      .The ending portion of these Vedas is called Upanishad.\nHindi:        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\nPred. Hindi:  इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।\n\nNo.  3\nEnglish:      Category: Religious Text\nHindi:        श्रेणी:धर्मग्रन्थ\nPred. Hindi:  श्रेणीधर्मग्रन्थ\n\nNo.  10\nEnglish:      Maine\nHindi:        मेन\nPred. Hindi:  मेन\n\nNo.  15\nEnglish:      category:information technology\nHindi:        श्रेणी:सूचना प्रौद्योगिकी\nPred. Hindi:  श्रेणीसूचना प्रौद्योगिकी\n\nNo.  18\nEnglish:      Aryans did not make any statues or temples for deities.\nHindi:        आर्य देवताओं की कोई मूर्ति या मन्दिर नहीं बनाते थे।\nPred. Hindi:  आर्य देवताओं की कोई मूर्ति या मन्दिर नहीं बनाते थे।\n\nNo.  19\nEnglish:      .Sarojini Naidu with Mahatma Gandhi\nHindi:        महात्मा गांधी के साथ सरोजिनी नायडू\nPred. Hindi:  महात्मा गांधी के साथ सरोजिनी नायडू\n\nNo.  21\nEnglish:      External links\nHindi:        बाहरी कड़ियाँ\nPred. Hindi:  बाहरी कड़ियाँ\n\nNo.  23\nEnglish:      This change has contributed\nHindi:        यह परिवर्तन योगदान दे रहा है\nPred. Hindi:  यह परिवर्तन योगदान दे रहा है\n\nNo.  24\nEnglish:      Buddha and danceroom (1958)\nHindi:        बुद्ध और नाचघर (1958)\nPred. Hindi:  बुद्ध और नाचघर\n\nNo.  26\nEnglish:      the Carrier Hotel\nHindi:        कैरियर होटल\nPred. Hindi:  कैरियर होटल\n\nNo.  28\nEnglish:      parliaments control on financial health.\nHindi:        वित्त व्यवस्था पर संसद का नियंत्रण\nPred. Hindi:  वित्त व्यवस्था पर संसद का नियंत्रण\n\nNo.  33\nEnglish:      Class : United States of America\nHindi:        श्रेणी:संयुक्त राज्य अमेरिका\nPred. Hindi:  श्रेणीसंयुक्त राज्य अमेरिका\n\nNo.  34\nEnglish:      permanent, billowing, voluptuous forms\nHindi:        स्थायी, तरंगित, आकर्षक, बड़ी इमारतो के आकार के\nPred. Hindi:  स्थायी तरंगित आकर्षक बड़ी इमारतो के द्वारा उत्पादित थी\n\nNo.  36\nEnglish:      Vishnusahastranam - Glory of Vishnu's 1000 names in Shantiparva.\nHindi:        विष्णुसहस्रनाम विष्णु के १००० नामों की महिमा शांतिपर्व में।\nPred. Hindi:  विष्णुसहस्रनाम विष्णु के १००० नामों की महिमा शांतिपर्व में।\n\nNo.  38\nEnglish:      The blue and the dim\nHindi:        नीले और मंद\nPred. Hindi:  नीले और मंद\n\nNo.  45\nEnglish:      Royal Chitvan National Park\nHindi:        रॉयल चितवन राष्ट्रीय उद्यान\nPred. Hindi:  रॉयल चितवन राष्ट्रीय उद्यान\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint.save(file_prefix = checkpoint_prefix)","execution_count":193,"outputs":[{"output_type":"execute_result","execution_count":193,"data":{"text/plain":"'./training_checkpoints/ckpt-1'"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}